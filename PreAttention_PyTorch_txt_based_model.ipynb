{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreAttention_PyTorch_txt_based_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "hrC0f3LFBEGv",
        "CKRNKTFbqBSL",
        "hpkyGaaig9-k"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrentBrick/PAE/blob/master/PreAttention_PyTorch_txt_based_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "A_jr8-iZGH9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data conversion to integers and keras models are at the bottom of the whole script!!\n"
      ]
    },
    {
      "metadata": {
        "id": "QV9li8zR9-hv",
        "colab_type": "code",
        "outputId": "199b7c58-798f-4aad-80f4-783fa2bdd33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #, force_remount=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KstlzmyN-Eat",
        "colab_type": "code",
        "outputId": "7dda0466-a0cb-4746-c202-0e5f330105a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive'\n",
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1kWIGJNa-GhD",
        "colab_type": "code",
        "outputId": "007eabdc-5759-48f8-8f8a-b62dcb134572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "cd My\\ Drive"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'My Drive'\n",
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ap4yp_TejsKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Start"
      ]
    },
    {
      "metadata": {
        "id": "C0oTjYUdjjeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "primary_train = pickle.load(open('training_primary_ints.pickle', 'rb'))#+1\n",
        "primary_test = pickle.load(open('testing_primary_ints.pickle', 'rb'))#+1\n",
        "primary_cv = pickle.load(open('validation_primary_ints.pickle', 'rb'))#+1\n",
        "\n",
        "#sequence_of_int = pickle.load(open('sequence_of_int.pickle', 'rb'))\n",
        "#pickle.dump(sequence_of_int, open('sequence_of_int.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHVMT5q1o5eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "90ee6208-ed30-4280-d18f-376d37787fee"
      },
      "cell_type": "code",
      "source": [
        "lens=[]\n",
        "for p in primary_train:\n",
        "    if p.shape[0] > 600:\n",
        "        lens.append(p.shape[0])\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "    \n",
        "plt.hist(lens, label='numbers at each length')\n",
        "plt.hlines(256,0,1000)\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BIVy9N4TpBZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cxu4llDK2zEF",
        "colab_type": "code",
        "outputId": "67fe8ea8-06b9-41b7-a9ab-27bb02236a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "primary_test.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "LSco3uDAWcJI",
        "colab_type": "code",
        "outputId": "9a6b46f6-bc6a-4bcc-dd37-50f4d084d8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "cell_type": "code",
      "source": [
        "primary_train[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16,  2, 13,  0,  0,  4,  2, 16, 11,  7, 17, 16,  9, 16, 14,  4, 17,\n",
              "       10,  3,  3,  5, 10,  8,  0, 14,  5, 16,  5,  3, 10, 16, 13,  9,  9,\n",
              "       11, 15,  9,  1, 16,  0, 17,  8,  0,  7, 15, 16,  0, 17, 14,  8,  0,\n",
              "        5,  7,  0,  6,  9, 19,  5,  7,  0,  5, 15, 16, 11, 17, 16,  5,  2,\n",
              "       13, 17,  8,  8,  9,  2, 17,  9, 15, 11,  2,  9, 17,  7, 11, 17,  9,\n",
              "        8, 15, 15,  4,  0, 16,  1, 17,  9, 17, 15,  3,  3,  2,  8, 11,  0,\n",
              "        7,  7, 17,  3, 12,  3,  8, 14,  5,  8, 19, 17, 17,  1,  4,  2, 12,\n",
              "        9,  2,  5, 15, 15, 11,  7,  2,  1,  9, 17, 15,  7,  5, 16,  7,  4,\n",
              "        5,  7, 19, 14,  8, 11, 15, 16,  2,  3, 12, 15,  3,  8,  2,  0,  9,\n",
              "       13, 12,  5, 14, 11,  9, 17,  0,  0,  5, 19,  0,  9, 19,  5, 15,  0,\n",
              "       16, 10,  9, 17,  9,  0, 10, 17, 11,  5, 17, 11,  1,  4, 10,  9,  2,\n",
              "       12,  0,  7,  5,  3,  4,  7,  9, 17,  2, 14,  2, 17,  8,  7,  8,  8,\n",
              "        8,  5, 15,  7, 19, 15,  7, 11,  3,  5, 19,  0,  8,  3,  4,  2, 12,\n",
              "        0,  7, 16,  3, 19,  7, 13, 14,  8,  8,  4, 12, 12,  2, 11, 15,  0,\n",
              "       12, 19,  5,  0, 14, 19, 17,  5, 15, 10, 17,  0,  2, 17,  6, 14, 16,\n",
              "        9, 17, 19,  5,  5,  7,  4, 10, 19, 12,  0, 11,  8,  8, 15, 12,  8,\n",
              "        5,  8,  9, 14,  9,  9, 19,  3,  1, 11, 12, 10,  0, 19, 17, 10,  3,\n",
              "        8,  0,  5,  5,  9,  0, 16, 16,  5,  8,  3,  0, 17,  9,  2,  7, 17,\n",
              "       12, 16,  2,  7,  6, 13, 14,  0, 12,  7,  7,  9,  5, 15, 12,  3,  2,\n",
              "       17, 16,  3,  9,  9,  3,  7, 19, 13,  8,  6,  0,  0,  8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "a5wQC7kW6paO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "lengths = []\n",
        "for s in primary_train: \n",
        "    lengths.append(len(s))\n",
        "import numpy as np\n",
        "#average length\n",
        "sum(lengths)/len(lengths)\n",
        "primary_train = primary_train[np.array(lengths) < 1000]\n",
        "\n",
        "\n",
        "lengths = []\n",
        "for s in primary_cv: \n",
        "    lengths.append(len(s))\n",
        "import numpy as np\n",
        "#average length\n",
        "sum(lengths)/len(lengths)\n",
        "primary_cv = primary_cv[np.array(lengths) < 1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YpgavMc4qi5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea326370-6fc4-4c4a-8c33-1d20bb977071"
      },
      "cell_type": "code",
      "source": [
        "primary_cv.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "ySQ6sXOlJm66",
        "colab_type": "code",
        "outputId": "16fc6189-0cb6-40cb-9e98-3feedd472a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "primary_train[1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15,  2,  9, 12,  0,  9, 15, 16,  5,  9,  9,  6,  9,  6, 13, 11,  7,\n",
              "       17,  2, 17, 13, 19, 10, 19,  5,  9, 15, 12,  0,  7, 16,  8, 19, 17,\n",
              "       17, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "VTIQlPQ3ay5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html \n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "# I DONT ACTUALLY USE ANY OF THIS!!!!\n",
        "\n",
        "class PrimarySequences(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.sequences = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.sequences[idx].shape[0] \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QYcrj9R_eJQx",
        "colab_type": "code",
        "outputId": "72f0d088-aaeb-4c26-8935-f85ca9882253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "primary_train[0].shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(337,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "1mZIk-inb9sY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train = PrimarySequences(primary_train)\n",
        "df_cv = PrimarySequences(primary_cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "urElYMCq_3Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://discuss.pytorch.org/t/tensorflow-esque-bucket-by-sequence-length/41284/4 buckets things of only the exact same size. \n",
        "\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "class BucketDataset(Sampler):\n",
        "    # want inputs to be an array\n",
        "    def __init__(self, inputs, batch_size):\n",
        "        self.inputs = inputs      # shape = (N, max_seq_len)\n",
        "        self.targets = None    # shape = (N, ) or None (e.g., for autoencoder I can simply use inputs)\n",
        "        self.batch_size = batch_size\n",
        "        ind_n_len = []\n",
        "        for i, p in enumerate(inputs):\n",
        "            ind_n_len.append( (i, p.shape[0]) )\n",
        "        self.ind_n_len = ind_n_len\n",
        "        \n",
        "    def _generate_batch_map(self, equal_length=False):\n",
        "        \n",
        "        shuffle(self.ind_n_len) # shuffle all of the indices first so they are put into buckets differently\n",
        "        \n",
        "        batch_map = OrderedDict()\n",
        "        # Organize lengths, e.g., batch_map[10] = [30, 124, 203, ...] <= indices of sequences of length 10\n",
        "        for idx, length in self.ind_n_len:\n",
        "            if length not in batch_map:\n",
        "                batch_map[length] = [idx]\n",
        "            else:\n",
        "                batch_map[length].append(idx)\n",
        "        # Use batch_map to split indices into batches of equal size\n",
        "        # e.g., for batch_size=3, batch_list = [[23,45,47], [49,50,62], [63,65,66], ...]\n",
        "        batch_list = []\n",
        "        for length, indices in batch_map.items():\n",
        "            for group in [indices[i:(i+self.batch_size)] for i in range(0, len(indices), self.batch_size)]:\n",
        "                batch_list.append(group)\n",
        "        return batch_list\n",
        "\n",
        "    def batch_count(self):\n",
        "        return len(self.batch_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lengths)\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        batch_list = self._generate_batch_map()\n",
        "        \n",
        "        shuffle(batch_list) # shuffle all the batches so they arent ordered by bucket size\n",
        "        for i in batch_list: \n",
        "            yield i \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1wuO58Sf36f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hotter(DataLoaderBatch):\n",
        "    \n",
        "    batch_split = list(zip(*DataLoaderBatch))\n",
        "    seqs, lengths = batch_split[0], batch_split[1]\n",
        "    \n",
        "    vocab_size = 20\n",
        "    batch_size = len(DataLoaderBatch)\n",
        "    max_length = lengths[0] # as all the lengths are the same in my case here! else take the max(lengths)\n",
        "    \n",
        "    y = torch.tensor(seqs).type(torch.LongTensor) #had device\n",
        "    y_onehot = torch.FloatTensor(batch_size, max_length, vocab_size)\n",
        "    y_onehot.zero_()\n",
        "    y_onehot.scatter_(2, torch.unsqueeze(y, 2), 1.0)\n",
        "    #print('max')\n",
        "    #print(y_onehot.max(dim=2)[1])\n",
        "    return y_onehot, torch.tensor(lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtWoouzqA_yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder"
      ]
    },
    {
      "metadata": {
        "id": "9sHYAuiladdT",
        "colab_type": "code",
        "outputId": "81a0cd43-069e-4248-d9f2-c29a14675d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class EncoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self, ENCODING_LSTM_OUTPUT=100, CODE_LAYER_SIZE=50, VOCAB_SIZE=20, ENCODER_LSTM_NUM_LAYERS=2 ):\n",
        "        super(EncoderNet, self).__init__()\n",
        "        #encoding\n",
        "        self.LSTM_NUM_LAYERS = ENCODER_LSTM_NUM_LAYERS\n",
        "        self.ENCODING_LSTM_OUTPUT = ENCODING_LSTM_OUTPUT\n",
        "        self.encoder = nn.LSTM(input_size=VOCAB_SIZE, hidden_size= self.ENCODING_LSTM_OUTPUT,num_layers=self.LSTM_NUM_LAYERS,bidirectional=True, batch_first=True)\n",
        "        #self.batchnorm = nn.BatchNorm1d(ENCODING_LSTM_OUTPUT * 2* self.LSTM_NUM_LAYERS) # as either 2 layers OR I concat them\n",
        "        self.dense1_enc = nn.Linear(in_features=(ENCODING_LSTM_OUTPUT* 2 ), out_features=CODE_LAYER_SIZE ) # * self.LSTM_NUM_LAYERS because it is bidirectional\n",
        "        self.dense2_enc = nn.Linear(in_features=CODE_LAYER_SIZE, out_features=CODE_LAYER_SIZE )\n",
        "        if self.LSTM_NUM_LAYERS >2:\n",
        "            print('WARNING, LSTM ENCODER HAS MORE THAN 2 LAYER, will need to change how hidden layers are concatenated!')\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        #print('encoder input', input)\n",
        "        #print('sequence length', seq_len)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        #res = torch.nn.utils.rnn.pack_padded_sequence(input, seq_len, batch_first=True).cuda()\n",
        "        res, (hidden_h, hidden_c) = self.encoder(input)\n",
        "        #res = torch.nn.utils.rnn.pad_packed_sequence(res, batch_first=True).cuda() # only use if I want the output w/ all timesteps for the LSTM\n",
        "        # combine the bidirectional outputs, concatenate states from separate layers. \n",
        "        #print('hidden encoder that i concat', hidden_h.shape)\n",
        "        \n",
        "        res = torch.mean(res, dim=1)\n",
        "        \n",
        "        #res = res.view(self.LSTM_NUM_LAYERS, 2, input.shape[0], self.ENCODING_LSTM_OUTPUT)\n",
        "        \n",
        "        '''if self.LSTM_NUM_LAYERS ==2:\n",
        "            #res = hidden_h.view(self.LSTM_NUM_LAYERS, 2, input.shape[0], self.ENCODING_LSTM_OUTPUT)\n",
        "            \n",
        "            # may be much more efficient ways to do this:\n",
        "            res = torch.cat( (torch.cat( (res[0,0,:,:], res[0,1,:,:]) ,1), torch.cat( (res[1,0,:,:], res[1,1,:,:]) ,1) ), 1)\n",
        "            #res= torch.cat( (torch.add(torch.mean(res[0,:,:], dim=),hidden_h[1,:,:]),torch.add(hidden_h[2,:,:],hidden_h[3,:,:])), 1)\n",
        "        else: \n",
        "            # average over all of the hidden states!\n",
        "            print(res.shape)\n",
        "            # res = torch.mean( res, dim=1)\n",
        "            torch.cat((hidden_h[0,:,:],hidden_h[1,:,:]), 1) '''\n",
        "        #res = self.batchnorm(res)\n",
        "        res = self.dense2_enc(F.elu(self.dense1_enc(res))) # used to have F.tanh here!\n",
        "        return res\n",
        "\n",
        "encoder_net = EncoderNet()\n",
        "print(encoder_net)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderNet(\n",
            "  (encoder): LSTM(20, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (dense1_enc): Linear(in_features=200, out_features=50, bias=True)\n",
            "  (dense2_enc): Linear(in_features=50, out_features=50, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z897GABlJQIv",
        "colab_type": "code",
        "outputId": "b46f0003-c1fe-43b7-d008-77443fa8fa55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "class DecoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self, DECODING_LSTM_OUTPUT=100, CODE_LAYER_SIZE=50, VOCAB_SIZE=20, DECODER_LSTM_NUM_LAYERS=1 ):\n",
        "        super(DecoderNet, self).__init__()\n",
        "        #decode it should be the inverse of the encoder!\n",
        "        #self.dense1_predecode = nn.Linear(in_features=(CODE_LAYER_SIZE), out_features=50 )\n",
        "        #self.dense1_pre_dec = nn.Linear(in_features=(CODE_LAYER_SIZE), out_features=200 )\n",
        "        self.LSTM_OUTPUT_SIZE = DECODING_LSTM_OUTPUT\n",
        "        #self.decoder_out = nn.LSTM(input_size=int(CODE_LAYER_SIZE/2),bidirectional=False, hidden_size=VOCAB_SIZE,num_layers=1, batch_first=True)\n",
        "        self.LSTM_NUM_LAYERS=DECODER_LSTM_NUM_LAYERS\n",
        "        self.CODE_LAYER_SIZE=CODE_LAYER_SIZE\n",
        "        self.VOCAB_SIZE=VOCAB_SIZE\n",
        "        self.decoder = nn.LSTM(input_size=CODE_LAYER_SIZE,bidirectional=True, \n",
        "                               hidden_size=self.LSTM_OUTPUT_SIZE,num_layers=self.LSTM_NUM_LAYERS, batch_first=True)\n",
        "        # should be half the size but the LSTM is bidirectional. \n",
        "        #self.batchnorm = nn.BatchNorm1d(self.LSTM_OUTPUT_SIZE)\n",
        "        #self.dense1_post_dec = nn.Linear(in_features=(self.LSTM_OUTPUT_SIZE), out_features=50 )\n",
        "        self.dense2_post_dec = nn.Linear(in_features=self.LSTM_OUTPUT_SIZE*2, out_features=VOCAB_SIZE )\n",
        "        #self.dense3_post_dec = nn.Linear(in_features=50, out_features=VOCAB_SIZE )\n",
        "\n",
        "    def forward(self, latent_space, prev_out, hidden):\n",
        "        #decoding. takes in a single latent code from a single part of the batch. \n",
        "        # where input is the teacher forcing or the predictions from the previous step.\n",
        "\n",
        "        prev_out, hidden = self.decoder(latent_space)\n",
        "        \n",
        "        #print('batch norm ', prev_out.shape)\n",
        "        #prev_out = self.batchnorm(prev_out.permute([0,2,1])).permute([0,2,1]).contiguous()\n",
        "        #print('batch norm ', prev_out.shape)\n",
        "        #prev_out = F.elu(self.dense1_post_dec(prev_out))\n",
        "        prev_out = F.log_softmax(self.dense2_post_dec(prev_out), dim=2)\n",
        "        \n",
        "        return prev_out, hidden\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        hidden_h = torch.empty([self.LSTM_NUM_LAYERS,batch_size,int(self.LSTM_OUTPUT_SIZE)], device=device)\n",
        "        torch.nn.init.orthogonal_(hidden_h, gain=nn.init.calculate_gain('tanh'))\n",
        "        return (hidden_h,\n",
        "                torch.zeros([self.LSTM_NUM_LAYERS,batch_size,int(self.LSTM_OUTPUT_SIZE)], device=device))\n",
        "\n",
        "'''   def initHidden_Out(self, batch_size):\n",
        "        return (torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device),\n",
        "                torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device))'''\n",
        "\n",
        "decoder_net = DecoderNet()\n",
        "print(decoder_net)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderNet(\n",
            "  (decoder): LSTM(50, 100, batch_first=True, bidirectional=True)\n",
            "  (dense2_post_dec): Linear(in_features=200, out_features=20, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y0jzD1cctdpm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Useful functions"
      ]
    },
    {
      "metadata": {
        "id": "l25UdNsstb4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQ9iRp3rDhno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def printParamNum(encoder_net, decoder_net):\n",
        "    params=0\n",
        "    for net in [encoder_net, decoder_net]:\n",
        "        model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "        params += sum([np.prod(p.size()) for p in net.parameters()])\n",
        "    print('total model parameters: ',params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAzY8aH7f_kw",
        "colab_type": "code",
        "outputId": "e7dff854-6c93-4643-8b19-99487c3f2e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''params=0\n",
        "net= encoder_net\n",
        "model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "params += sum([np.prod(p.size()) for p in net.parameters()])\n",
        "print('total model parameters: ',params)'''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"params=0\\nnet= encoder_net\\nmodel_parameters = filter(lambda p: p.requires_grad, net.parameters())\\nparams += sum([np.prod(p.size()) for p in net.parameters()])\\nprint('total model parameters: ',params)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "Px1G8Ag6A4I7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def saveModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, save_name, train_loss, eval_acc, e):\n",
        "    for name, net, optim in zip(['encoder_save','decoder_save'],[encoder_net, decoder_net],[encoder_optimizer, decoder_optimizer] ):\n",
        "        torch.save({\n",
        "                    'epoch': e,\n",
        "                    'model_state_dict': net.state_dict(),\n",
        "                    'optimizer_state_dict': optim.state_dict(),\n",
        "                    'last_loss': train_loss,\n",
        "                    'eval_accuracy':eval_acc\n",
        "                    }, save_name+name+'.tar')\n",
        "    print('saveModel worked')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvJ54q0mA6u-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loadModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, load_name, ignore_optim=False):\n",
        "    #ignore optim is for when I am loading in a model to assess predictions and not training it anymore. \n",
        "    for name, net, optim in zip(['encoder_save','decoder_save'],[encoder_net, decoder_net],[encoder_optimizer, decoder_optimizer] ):\n",
        "        checkpoint = torch.load(load_name+name+'.tar')\n",
        "        state = checkpoint['model_state_dict'] #.state_dict()\n",
        "        #state.update(net.state_dict())\n",
        "        net.load_state_dict(state)#checkpoint['model_state_dict'])\n",
        "        if not ignore_optim:\n",
        "            optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        e = checkpoint['epoch']\n",
        "        loss = checkpoint['last_loss']\n",
        "        best_eval_acc = checkpoint['eval_accuracy']\n",
        "    print('loaded in previous model!')\n",
        "    return encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, loss, e, best_eval_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vcgo4AaumR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        m.bias.data.fill_(0.01)\n",
        "    elif type(m) == nn.LSTM:\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                torch.nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain('tanh'))\n",
        "            if 'bias' in name:\n",
        "                param = torch.zeros(param.shape)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0ZFfPDw_J2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_weights(m):\n",
        "    save_init_weights = dict()\n",
        "    for name, param in m.named_parameters():\n",
        "        save_init_weights[name] = param\n",
        "    return save_init_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_himfbRAueM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy_one_hot(Y_hat, Y):\n",
        "    criterion = torch.nn.NLLLoss(size_average=True, ignore_index=-1)\n",
        "    loss=  criterion(Y_hat.permute([0,2,1]).contiguous(), Y.max(dim=2)[1])\n",
        "    #print('y shape', Y.shape)\n",
        "    \n",
        "    # flatten all the labels\n",
        "    Y = Y.max(dim=2)[1].view(-1)\n",
        "    Y_hat = Y_hat.view(-1, VOCAB_SIZE)\n",
        "    #mask = (Y > 0).float()\n",
        "    nb_tokens = Y.shape[0]#int(torch.sum(mask).item())\n",
        "    #get accuracy\n",
        "    top_preds = Y_hat.max(dim=1)[1]\n",
        "    acc = torch.sum( torch.eq(top_preds, Y).type(torch.cuda.FloatTensor))/nb_tokens  #*mask) / nb_tokens\n",
        "    #Y_hat = Y_hat[torch.arange(Y_hat.shape[0]), Y]* mask\n",
        "    \n",
        "    return loss, acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7loCa-I4xVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_forward(encoder_net, decoder_net, seqs, lengths, device, readout=True, teacher_forcing=False, make_preds=False, print_preds=False ):\n",
        "  \n",
        "    latent = encoder_net(seqs,lengths)\n",
        "    \n",
        "    batch_size = seqs.shape[0]\n",
        "    max_l = torch.max(lengths)\n",
        "    hidden = decoder_net.initHidden(batch_size)\n",
        "    \n",
        "    # need to give each of the latents a time step proportional to their length number. \n",
        "    # get rid of the time STEPS IF DOING TEACHER FORCING AND INCREMENT THE INPUTS BY ONE!\n",
        "    if(readout==True): \n",
        "\n",
        "        batch_outs = torch.zeros([batch_size,max_l, VOCAB_SIZE], device=device, requires_grad=False)\n",
        "        \n",
        "        # if another LSTM need to add this one and another one. \n",
        "        #hidden_out = decoder_net.initHidden_Out(batch_size)\n",
        "        \n",
        "        if (not teacher_forcing):\n",
        "            #time_steps=1\n",
        "            prev_out = torch.zeros([batch_size,1, 1], device=device, requires_grad=False)\n",
        "            latent = latent.view(latent.shape[0],1,latent.shape[1])\n",
        "            for t in range(max_l):\n",
        "                prev_out, hidden = decoder_net(latent, prev_out, hidden)\n",
        "                #print('is prev out requires grad? ', prev_out.requires_grad)\n",
        "                batch_outs[:,t,:]= prev_out.squeeze()\n",
        "                prev_out = prev_out.max(dim=2, keepdim=True)[1].to(torch.float32)\n",
        "                \n",
        "        else:\n",
        "            latent = latent.view(latent.shape[0],1,latent.shape[1]).expand(-1,max_l.item(),-1)\n",
        "            ground_truth = seqs.max(dim=2, keepdim=True)[1][:,:-1,:]\n",
        "            #prev_out = input[0].max(dim=2, keepdim=True)[1].to(torch.float32).to(device) # this was just seeing if the model would learn. \n",
        "            prev_out = torch.cat( (torch.zeros([ground_truth.shape[0], 1, 1]), ground_truth.to(torch.float32) ) ,1).to(device) #as both tensors should already be in cuda!\n",
        "            #print('shape of prev out', prev_out.shape)\n",
        "            #print('what does prev out look like? arranged right?', prev_out[0,:,:])\n",
        "            batch_outs, hidden = decoder_net(latent, prev_out, hidden)\n",
        "        # add zero padding to the end. Dont need to do as I am computing each batch now. \n",
        "        #batch_outs[b_ind,:,:].add_( torch.cat( ( seq_outs, torch.zeros([ (max_l-length) ,VOCAB_SIZE] , device=device, requires_grad=False) ) , 0))\n",
        "\n",
        "    else: \n",
        "        prev_out = torch.zeros([batch_size,max_l.item(), 1], device=device, requires_grad=False)\n",
        "        # this was for the original keras model where I needed to repeat vector the latent space. \n",
        "        batch_outs, hidden = decoder_net( latent.view(batch_size,1,-1).expand(-1,max_l.item(),-1),prev_out, hidden)\n",
        "\n",
        "    if make_preds:\n",
        "        return batch_outs\n",
        "    \n",
        "    if print_preds:\n",
        "        # always print a random number. \n",
        "        rand_ind = int(torch.rand([1]).item()*batch_size)\n",
        "        print('ground truth', seqs.max(dim=2)[1][rand_ind,:])\n",
        "        print('predicted values', batch_outs.max(dim=2)[1][rand_ind,:])\n",
        "    \n",
        "    # feed in the sequence length for each example and the truth\n",
        "    return cross_entropy_one_hot(batch_outs, seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFKoJDXKI-ft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Trainer"
      ]
    },
    {
      "metadata": {
        "id": "7hoO7VVxI8p6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def fitModel(encoder_net, decoder_net, encoder_optimizer, \n",
        "             decoder_optimizer, BATCH_SIZE, epochs, e, learning_rate, \n",
        "             mem_pin, device, save_name, load_name, readout, allow_teacher_force, \n",
        "             teaching_strategy, clip, want_preds_printed, encoder_scheduler, decoder_scheduler):\n",
        "    \n",
        "    print('device being used is: ', device)\n",
        "    print('teacher forcing?', allow_teacher_force)\n",
        "    print('teaching strategy', teaching_strategy)\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses_train = []\n",
        "    plot_losses_eval = []\n",
        "    print_loss_total = 0 \n",
        "    plot_loss_total = 0\n",
        "    running_loss = 0\n",
        "    prob_of_teacher_forcing = 0.7\n",
        "    #bad_loss_running = 0\n",
        "    print_every=1\n",
        "    plot_every=1\n",
        "    accuracy_running =0.0\n",
        "    num_batches_per_epoch = int(len(df_train)/BATCH_SIZE)\n",
        "    num_eval_batches_per_epoch = int(len(df_cv)/BATCH_SIZE)\n",
        "    accuracy=0.05*num_batches_per_epoch # need it for my teacher forcing calc. \n",
        "    best_eval_acc = 0.0\n",
        "    print('approximate num batches per ep', num_batches_per_epoch)\n",
        "    # how often to print minibatch loss\n",
        "    print_mini_every = 100\n",
        "\n",
        "    printParamNum(encoder_net, decoder_net)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    print('mem pinned? ', mem_pin)\n",
        "    \n",
        "    sampler = BucketDataset(primary_train, BATCH_SIZE)\n",
        "    dataloader = DataLoader(df_train, batch_size=1, \n",
        "                            batch_sampler=sampler, shuffle=False,\n",
        "                            num_workers=8, collate_fn=one_hotter, \n",
        "                            drop_last=False, pin_memory=mem_pin)\n",
        "\n",
        "    evaluate_sampler = BucketDataset(primary_cv, BATCH_SIZE)\n",
        "    evaluate_dataloader = DataLoader(df_cv, batch_size=1, \n",
        "                        batch_sampler=evaluate_sampler, shuffle=False,\n",
        "                        num_workers=8, collate_fn=one_hotter, \n",
        "                        drop_last=False, pin_memory=mem_pin)\n",
        "    #evaluate_dataloader = DataLoader(df_cv, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, collate_fn=pad_and_sort_batch, drop_last=False, pin_memory=mem_pin)\n",
        "    mini_batch_iters=0\n",
        "    \n",
        "    print('number of epochs', (epochs+e))\n",
        "    \n",
        "    init_e = e\n",
        "\n",
        "    while e < (epochs+init_e):  \n",
        "        print('Epoch', e)\n",
        "        ep_loss = 0.0\n",
        "        accuracy=0.0\n",
        "        num_batches_per_epoch =0\n",
        "\n",
        "        # iterate through data\n",
        "        for x in dataloader:\n",
        "            \n",
        "            seqs = x[0].to(device)\n",
        "            lengths = x[1].to(device)\n",
        "\n",
        "            num_batches_per_epoch += 1\n",
        "            \n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "\n",
        "            if allow_teacher_force:\n",
        "                rand_for_teacher_forcing = np.random.rand(1)[0]\n",
        "                teacher_force= True if rand_for_teacher_forcing < prob_of_teacher_forcing else False\n",
        "            else:\n",
        "                teacher_force= False\n",
        "                \n",
        "            loss, acc = train_forward(encoder_net, decoder_net, seqs, lengths, device, \n",
        "                                      teacher_forcing=teacher_force, readout=readout)#, print_preds=want_preds_printed)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip gradients: gradients are modified in place\n",
        "            encoder_clip = torch.nn.utils.clip_grad_norm_(encoder_net.parameters(), clip)\n",
        "            if encoder_clip > clip:\n",
        "                print('clipped encoder gradient with a value of: ', encoder_clip)\n",
        "            decoder_clip = torch.nn.utils.clip_grad_norm_(decoder_net.parameters(), clip)\n",
        "            if decoder_clip > clip:\n",
        "                print('clipped decoder gradient with a value of: ', decoder_clip)\n",
        "\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "            \n",
        "            accuracy += acc.item()\n",
        "            accuracy_running += acc.item()\n",
        "\n",
        "            print_loss_total += loss.item()\n",
        "            plot_loss_total += loss.item()\n",
        "\n",
        "            running_loss +=  loss.item()\n",
        "            ep_loss +=  loss.item()\n",
        "\n",
        "            mini_batch_iters+=1\n",
        "\n",
        "            if mini_batch_iters % print_mini_every == (print_mini_every-1):\n",
        "                print('# of minibatch iters:', (mini_batch_iters+1), 'prints every', print_mini_every, 'loss: ', \n",
        "                      round( (running_loss/print_mini_every), 4), 'accuracy', round((accuracy_running/print_mini_every),4) )\n",
        "                running_loss = 0.0\n",
        "                accuracy_running =0.0\n",
        "\n",
        "        print('Total loss for epoch ',e, 'is: ', round(ep_loss ,4))\n",
        "\n",
        "        if (e % print_every == 0):\n",
        "            #print('mini batch count', mini_batch_iters)\n",
        "            #print('number of batches per ep', num_batches_per_epoch)\n",
        "            print_loss_avg = (print_loss_total / print_every) / num_batches_per_epoch\n",
        "            plot_losses_train.append(print_loss_avg)\n",
        "            print_loss_total = 0\n",
        "            print('Time passed: %s Time Till Done: (%d %d%%) Loss average: %.4f Accuracy: %.4f' % (timeSince(start, e / epochs), e, e /epochs * 100, print_loss_avg, \n",
        "                                                                                                   accuracy/num_batches_per_epoch) )\n",
        "        # cross_eval of model: \n",
        "        with torch.no_grad():\n",
        "            encoder_net.eval()\n",
        "            decoder_net.eval()\n",
        "            tot_eval_loss = 0.0\n",
        "            tot_eval_acc = 0.0\n",
        "            num_eval_batches_per_epoch = 0 \n",
        "            for cv_batch in evaluate_dataloader:\n",
        "                \n",
        "                seqs = cv_batch[0].to(device)\n",
        "                lengths = cv_batch[1].to(device)\n",
        "                \n",
        "                num_eval_batches_per_epoch += 1\n",
        "                eval_loss, eval_acc = train_forward(encoder_net, decoder_net, seqs,lengths, device, teacher_forcing=False, readout=readout, print_preds=want_preds_printed)\n",
        "                tot_eval_loss+= eval_loss.item()\n",
        "                tot_eval_acc+= eval_acc.item()\n",
        "            tot_eval_acc = tot_eval_acc/num_eval_batches_per_epoch\n",
        "            tot_eval_loss = tot_eval_loss/num_eval_batches_per_epoch\n",
        "            plot_losses_eval.append(tot_eval_loss)\n",
        "            print('Eval Loss average per batch: %.4f Accuracy: %.4f' % (tot_eval_loss, tot_eval_acc) ) \n",
        "            if (accuracy/num_batches_per_epoch>best_eval_acc):\n",
        "                print('new best eval_accuracy! At:', round(tot_eval_acc,4),' Saving model')\n",
        "                best_eval_acc = accuracy/num_batches_per_epoch\n",
        "                saveModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, save_name, loss.item(), tot_eval_acc, e)\n",
        "        \n",
        "        encoder_net.train()\n",
        "        decoder_net.train()\n",
        "        \n",
        "        # determine probability of teacher forcing for the next epoch\n",
        "        if(allow_teacher_force and teaching_strategy =='accuracy'): \n",
        "            prob_of_teacher_forcing = 1-((accuracy/num_batches_per_epoch)+0.3)\n",
        "            print('prob use teacher forcing this epoch', round(prob_of_teacher_forcing,2))\n",
        "            \n",
        "        elif(allow_teacher_force and teaching_strategy=='epoch'):\n",
        "            prob_of_teacher_forcing = 0.7-(e/100)\n",
        "            print('prob use teacher forcing this epoch', round(prob_of_teacher_forcing,2))\n",
        "        \n",
        "        pickle.dump((plot_losses_train, plot_losses_eval), open(save_name+'list_of_losses_to_plot.pickle', 'wb'))\n",
        "        \n",
        "        encoder_scheduler.step(accuracy/num_batches_per_epoch)\n",
        "        decoder_scheduler.step(accuracy/num_batches_per_epoch)\n",
        "        \n",
        "        e +=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6es5N1hX9xO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Command Center"
      ]
    },
    {
      "metadata": {
        "id": "kSdRoNGJvkNr",
        "colab_type": "code",
        "outputId": "da39f512-4d53-4d6b-896d-2715ee095384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2686
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "ENCODING_LSTM_OUTPUT=800\n",
        "CODE_LAYER_SIZE=300\n",
        "DECODING_LSTM_OUTPUT=800\n",
        "VOCAB_SIZE=20\n",
        "ENCODER_LSTM_NUM_LAYERS=2\n",
        "DECODER_LSTM_NUM_LAYERS=2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "mem_pin = False\n",
        "BATCH_SIZE = 256\n",
        "epochs = 500\n",
        "curr_ep = 1 # cant be 0 else later on there is division by zero!\n",
        "learning_rate=0.0001\n",
        "clip=30\n",
        "\n",
        "readout=False\n",
        "allow_teacher_force = False\n",
        "teaching_strategy = 'epoch' # can also be 'accuracy'\n",
        "want_preds_printed = False\n",
        "\n",
        "encoder_net = EncoderNet(ENCODING_LSTM_OUTPUT=ENCODING_LSTM_OUTPUT, CODE_LAYER_SIZE=CODE_LAYER_SIZE, \n",
        "                         VOCAB_SIZE=VOCAB_SIZE, ENCODER_LSTM_NUM_LAYERS=ENCODER_LSTM_NUM_LAYERS).to(device)\n",
        "decoder_net = DecoderNet(DECODING_LSTM_OUTPUT=DECODING_LSTM_OUTPUT, CODE_LAYER_SIZE=CODE_LAYER_SIZE, \n",
        "                         VOCAB_SIZE=VOCAB_SIZE, DECODER_LSTM_NUM_LAYERS=DECODER_LSTM_NUM_LAYERS).to(device)\n",
        "\n",
        "encoder_optimizer = optim.RMSprop(encoder_net.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.RMSprop(decoder_net.parameters(), lr=learning_rate)\n",
        "\n",
        "#encoder_optimizer = optim.SGD(encoder_net.parameters(), lr=learning_rate, momentum =0.9)\n",
        "#decoder_optimizer = optim.SGD(decoder_net.parameters(), lr=learning_rate, momentum =0.9)\n",
        "\n",
        "# initialize the model weights! \n",
        "'''enc_saved_weights = dict()\n",
        "dec_saved_weights = dict()'''\n",
        "#nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n",
        "for net in [encoder_net, decoder_net]: #, save_dict in zip([encoder_net, decoder_net], [enc_saved_weights, dec_saved_weights]):\n",
        "    net.apply(init_weights)\n",
        "    #save_dict = net.apply(save_weights)\n",
        "\n",
        "#LOAD IN EXISTING MODEL? \n",
        "load_model =False\n",
        "save_name = 'code300_' \n",
        "load_name = 'code300'\n",
        "print('Name that all models for this train will be saved under:', save_name)\n",
        "if load_model:\n",
        "    print(\"LOADING IN A MODEL, load_model=True\")\n",
        "    encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, loss, curr_ep, best_eval_acc = loadModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, load_name)\n",
        "\n",
        "encoder_net.train()\n",
        "decoder_net.train()\n",
        "\n",
        "# WATCH OUT FOR MIN VS MAX!!!\n",
        "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'max', factor=0.5, patience=10, verbose=True, threshold=0.0001 )\n",
        "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'max', factor=0.5, patience=10, verbose=True, threshold=0.0001  )\n",
        "    \n",
        "fitModel(encoder_net, decoder_net, encoder_optimizer, decoder_optimizer, \n",
        "         BATCH_SIZE, epochs, curr_ep, learning_rate, mem_pin, device, \n",
        "         save_name, load_name, readout, allow_teacher_force, teaching_strategy, clip, want_preds_printed, encoder_scheduler, decoder_scheduler)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name that all models for this train will be saved under: simple_model_larger_encoder4_\n",
            "device being used is:  cuda\n",
            "teacher forcing? False\n",
            "teaching strategy epoch\n",
            "approximate num batches per ep 404\n",
            "total model parameters:  18883020\n",
            "mem pinned?  False\n",
            "number of epochs 501\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# of minibatch iters: 50 prints every 50 loss:  2.8619 accuracy 0.0855\n",
            "# of minibatch iters: 100 prints every 50 loss:  2.8951 accuracy 0.0958\n",
            "# of minibatch iters: 150 prints every 50 loss:  2.8727 accuracy 0.1055\n",
            "# of minibatch iters: 200 prints every 50 loss:  2.8601 accuracy 0.1095\n",
            "# of minibatch iters: 250 prints every 50 loss:  2.849 accuracy 0.1121\n",
            "# of minibatch iters: 300 prints every 50 loss:  2.8491 accuracy 0.1116\n",
            "# of minibatch iters: 350 prints every 50 loss:  2.8463 accuracy 0.1101\n",
            "# of minibatch iters: 400 prints every 50 loss:  2.8379 accuracy 0.1123\n",
            "# of minibatch iters: 450 prints every 50 loss:  2.8499 accuracy 0.1107\n",
            "# of minibatch iters: 500 prints every 50 loss:  2.8391 accuracy 0.1118\n",
            "# of minibatch iters: 550 prints every 50 loss:  2.8382 accuracy 0.1113\n",
            "# of minibatch iters: 600 prints every 50 loss:  2.844 accuracy 0.1102\n",
            "# of minibatch iters: 650 prints every 50 loss:  2.8281 accuracy 0.1141\n",
            "# of minibatch iters: 700 prints every 50 loss:  2.8427 accuracy 0.1085\n",
            "# of minibatch iters: 750 prints every 50 loss:  2.8175 accuracy 0.1159\n",
            "# of minibatch iters: 800 prints every 50 loss:  2.8165 accuracy 0.1165\n",
            "# of minibatch iters: 850 prints every 50 loss:  2.8062 accuracy 0.1197\n",
            "# of minibatch iters: 900 prints every 50 loss:  2.8255 accuracy 0.1141\n",
            "# of minibatch iters: 950 prints every 50 loss:  2.8175 accuracy 0.1173\n",
            "# of minibatch iters: 1000 prints every 50 loss:  2.8251 accuracy 0.1154\n",
            "# of minibatch iters: 1050 prints every 50 loss:  2.8173 accuracy 0.1166\n",
            "# of minibatch iters: 1100 prints every 50 loss:  2.828 accuracy 0.1148\n",
            "Total loss for epoch  1 is:  3131.8226\n",
            "Time passed: 18m 54s (- 9434m 58s) Time Till Done: (1 0%) Loss average: 2.8419 Accuracy: 0.1110\n",
            "Eval Loss average per batch: 2.7568 Accuracy: 0.1332\n",
            "new best eval_accuracy! At: 0.1332  Saving model\n",
            "saveModel worked\n",
            "Epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-f1a01c36079c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m fitModel(encoder_net, decoder_net, encoder_optimizer, decoder_optimizer, \n\u001b[1;32m     59\u001b[0m          \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_pin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m          save_name, load_name, readout, allow_teacher_force, teaching_strategy, clip, want_preds_printed, encoder_scheduler, decoder_scheduler)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-84765c9713e2>\u001b[0m in \u001b[0;36mfitModel\u001b[0;34m(encoder_net, decoder_net, encoder_optimizer, decoder_optimizer, BATCH_SIZE, epochs, e, learning_rate, mem_pin, device, save_name, load_name, readout, allow_teacher_force, teaching_strategy, clip, want_preds_printed, encoder_scheduler, decoder_scheduler)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                       teacher_forcing=teacher_force, readout=readout)#, print_preds=want_preds_printed)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Clip gradients: gradients are modified in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "E5YAmcdl7wAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saveModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, save_name, 1.59, 0.9, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlSk7byCOAe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_name = 'simple_model_larger_encoder3_' \n",
        "plot_losses_train, plot_losses_eval=pickle.load(open(save_name+'list_of_losses_to_plot.pickle', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxpcalqAOH1c",
        "colab_type": "code",
        "outputId": "8e216361-5a5f-4bf8-b9ae-e2de741b2ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "    \n",
        "plt.plot(plot_losses_train, label='train loss')\n",
        "plt.plot(plot_losses_eval, label='eval loss')\n",
        "plt.show()"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PCfj3X7Ge3U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXoCCpAxAEVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0UlvsnmHCjh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpustat -p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vskp_Ko0p86q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# See how predictions are: "
      ]
    },
    {
      "metadata": {
        "id": "ZLDRjNhAqAje",
        "colab_type": "code",
        "outputId": "fcd357d5-2621-4142-9512-db2097301b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "load_name = 'simple_model_larger_encoder3_'\n",
        "\n",
        "ENCODING_LSTM_OUTPUT=500\n",
        "CODE_LAYER_SIZE=200\n",
        "DECODING_LSTM_OUTPUT=500\n",
        "#ENCODING_LSTM_OUTPUT=200\n",
        "#CODE_LAYER_SIZE=200\n",
        "VOCAB_SIZE=21\n",
        "ENCODER_LSTM_NUM_LAYERS=2\n",
        "DECODER_LSTM_NUM_LAYERS=2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "mem_pin = False\n",
        "BATCH_SIZE = 32\n",
        "epochs = 80\n",
        "curr_ep = 1 # cant be 0 else later on there is division by zero!\n",
        "learning_rate=0.001\n",
        "clip=15\n",
        "\n",
        "readout=False\n",
        "allow_teacher_force = False\n",
        "teaching_strategy = 'epoch' # can also be 'accuracy'\n",
        "\n",
        "encoder_net = EncoderNet(ENCODING_LSTM_OUTPUT=ENCODING_LSTM_OUTPUT, CODE_LAYER_SIZE=CODE_LAYER_SIZE, \n",
        "                         VOCAB_SIZE=VOCAB_SIZE, ENCODER_LSTM_NUM_LAYERS=ENCODER_LSTM_NUM_LAYERS).to(device)\n",
        "decoder_net = DecoderNet(DECODING_LSTM_OUTPUT=DECODING_LSTM_OUTPUT, CODE_LAYER_SIZE=CODE_LAYER_SIZE, \n",
        "                         VOCAB_SIZE=VOCAB_SIZE, DECODER_LSTM_NUM_LAYERS=DECODER_LSTM_NUM_LAYERS).to(device)\n",
        "encoder_optimizer = optim.RMSprop(encoder_net.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.RMSprop(decoder_net.parameters(), lr=learning_rate)\n",
        "\n",
        "encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, loss, e, best_eval_acc = loadModel(encoder_net, decoder_net,encoder_optimizer, decoder_optimizer, load_name, ignore_optim=True)"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded in previous model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZMIQI0MqQ9N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampler = BucketDataset(primary_train, BATCH_SIZE)\n",
        "dataloader = DataLoader(df_train, batch_size=1, \n",
        "                        batch_sampler=sampler, shuffle=False,\n",
        "                        num_workers=8, collate_fn=one_hotter, \n",
        "                        drop_last=False, pin_memory=mem_pin)\n",
        "\n",
        "# Custom buckets and batch size because the CV dataset is so much smaller. \n",
        "evaluate_sampler = BucketDataset(primary_cv, BATCH_SIZE) # ,100,150,200,300,400\n",
        "evaluate_dataloader = DataLoader(df_cv, batch_size=1, \n",
        "                    batch_sampler=evaluate_sampler, shuffle=False,\n",
        "                    num_workers=8, collate_fn=one_hotter, \n",
        "                    drop_last=False, pin_memory=mem_pin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OlfESW5hqTU_",
        "colab_type": "code",
        "outputId": "e8e226bf-59f4-4b6c-e31d-583d203c1979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for cv_batch in dataloader:#evaluate_dataloader: \n",
        "        \n",
        "        seqs = cv_batch[0].to(device)\n",
        "        lengths = cv_batch[1].to(device)\n",
        "        print(encoder_net(seqs, lengths))\n",
        "        break\n",
        "\n",
        "        eval_loss, eval_acc = train_forward(encoder_net, decoder_net, seqs,lengths, device, teacher_forcing=False, readout=readout, print_preds=True)\n",
        "\n",
        "        #preds = train_forward(encoder_net, decoder_net, cv_batch, device, teacher_forcing=False, make_preds=True)\n",
        "        #target = (cv_batch[0]).to(device)\n",
        "        #seq_lengths = cv_batch[1]"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-400-7b9005cd326f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-372-328f3951f73f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, seq_len)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Pack padded batch of sequences for RNN module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#res = torch.nn.utils.rnn.pack_padded_sequence(input, seq_len, batch_first=True).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#res = torch.nn.utils.rnn.pad_packed_sequence(res, batch_first=True).cuda() # only use if I want the output w/ all timesteps for the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# combine the bidirectional outputs, concatenate states from separate layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    133\u001b[0m             raise RuntimeError(\n\u001b[1;32m    134\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 135\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 21, got 20"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JzzJKqthigxg",
        "colab_type": "code",
        "outputId": "1dc54ed1-0cec-4279-b2ba-403a67ac5066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for cv_batch in evaluate_dataloader: \n",
        "        print(encoder_net(cv_batch[0],cv_batch[1]))\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0023,  0.2183,  0.2718,  ..., -0.2392, -0.1695,  0.3490],\n",
            "        [ 0.3124, -0.2415, -0.0100,  ..., -0.0135,  0.2223,  0.2753],\n",
            "        [-0.0847, -0.3245,  0.2076,  ..., -0.1262,  0.4366, -0.0182],\n",
            "        ...,\n",
            "        [ 0.2962, -0.2558, -0.5093,  ..., -0.0022,  0.4100, -0.2511],\n",
            "        [ 0.0118, -0.0047,  0.0222,  ...,  0.0563,  0.0715, -0.0719],\n",
            "        [ 0.2619, -0.0094, -0.0852,  ...,  0.0934,  0.2698, -0.2786]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9Umi9LLqFzQ",
        "colab_type": "code",
        "outputId": "f62259fa-fdb5-4edc-dab4-c9767fedfe5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "cell_type": "code",
      "source": [
        "ind=1\n",
        "lens = seq_lengths[ind].item()\n",
        "pred = torch.max(preds[ind,:,:], dim=1)[1][:lens]\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([17, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19,\n",
              "        14, 14, 14, 19, 19, 19, 14, 14, 19, 19, 14, 19, 14, 14, 19, 19, 14, 19,\n",
              "        19, 14, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 19, 14, 14, 19, 19, 19,\n",
              "        19, 14, 14, 14, 14, 19, 19, 19, 19, 14, 14, 19, 14, 19, 19, 19, 14, 14,\n",
              "        14, 19, 19, 19, 14, 19, 14, 14, 19, 19, 14, 19, 14, 19, 14, 19, 19, 19,\n",
              "        14, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14, 19, 19, 14, 19,\n",
              "        14, 19, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 19, 19, 14, 14, 19, 19,\n",
              "        14, 19, 14, 19, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 19, 14, 14, 14,\n",
              "        19, 19, 14, 19, 14, 19, 14, 19, 19, 19, 14, 14, 19, 19, 14, 14, 19, 19,\n",
              "        14, 19, 14, 19, 14, 19, 19, 19, 14, 14, 14, 19, 14, 19, 19, 19, 14, 14,\n",
              "        19, 19, 14, 14, 19, 19, 14, 19, 19, 14, 14, 19, 19, 14, 14, 19, 19, 14,\n",
              "        14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 19, 14, 14, 19, 19, 14, 14,\n",
              "        19, 19, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14, 19, 19, 14, 19,\n",
              "        14, 19, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14,\n",
              "        19, 19, 14, 19, 14, 19, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 19, 19,\n",
              "        14, 14, 19, 19, 14, 19, 19, 14, 14, 14, 19, 19, 19, 19, 14, 14, 14, 19,\n",
              "        19, 19, 14, 14, 14, 19, 19, 19, 14, 19, 14, 14, 19, 19, 14, 19, 14, 19,\n",
              "        19, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14, 19, 19, 14, 14,\n",
              "        19, 19, 14, 19, 19, 19, 14, 14, 14, 19, 14, 19, 19, 19, 14, 14, 19, 19,\n",
              "        14, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 19, 14, 19, 19, 14, 14, 19,\n",
              "        19, 19, 14, 14, 14, 19, 19, 19, 19, 14, 14, 14, 19, 14, 19, 19, 14, 19,\n",
              "        19, 14, 14, 19, 19, 19, 14, 14, 19, 14, 19, 19, 14, 19, 14, 19, 14, 19,\n",
              "        14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 19,\n",
              "        14, 19, 14, 19, 14, 19, 14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 14,\n",
              "        14, 19, 19, 19, 14, 14, 19, 19, 14, 14, 19, 19, 19, 14, 14, 19, 14, 19,\n",
              "        14, 19, 19, 19, 14, 14, 14, 19, 19, 19, 14, 19, 14, 14, 19, 19, 14, 19],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "YTJgDPAzj6g6",
        "colab_type": "code",
        "outputId": "18154e2e-e85d-468e-fbf6-5de3e3900647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([468])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "metadata": {
        "id": "R2dmcJ2cqH8_",
        "colab_type": "code",
        "outputId": "a214477f-e1d5-42d4-a016-a825af9848e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cv_batch[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 595, 21])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "g5Q5Y7C9qAe8",
        "colab_type": "code",
        "outputId": "0e3b43d5-c6f8-4647-d1d6-11c746e50c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "cell_type": "code",
      "source": [
        "true = torch.max(cv_batch[0][ind,:,:], dim=1)[1][:lens]\n",
        "true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([11,  6, 16, 16,  7,  7,  7,  7,  7,  7, 11, 16, 16, 10,  9,  5, 18, 18,\n",
              "        16, 16, 10,  3,  8,  8,  1,  1, 14,  1,  6, 15, 12,  9, 14, 10,  1,  4,\n",
              "        10,  1,  4,  9,  1, 10,  1,  1,  8,  9,  4, 12,  3, 14, 14, 10, 13,  3,\n",
              "        13,  4, 18, 18,  5,  1, 13, 10, 14, 10,  1, 17,  9, 16,  6, 17,  8, 13,\n",
              "        10, 17, 17, 17,  1, 10,  3,  2,  8,  6,  9, 10,  8, 16, 20, 16, 20,  5,\n",
              "        16,  1, 13, 16, 16, 16,  1, 17, 14,  3,  6, 17,  4, 14, 17, 13, 10,  8,\n",
              "         4, 15,  1,  8,  3, 17,  8,  2,  3,  2,  5, 14,  6,  4, 17, 17, 10, 18,\n",
              "         4,  8, 14, 10, 14,  8, 18,  9, 16, 10, 10,  1,  1, 18, 10, 12,  3,  9,\n",
              "         8,  8, 18,  7,  6,  1,  6, 10, 10,  9,  1, 18, 15, 14, 18, 20, 12,  8,\n",
              "         5, 10, 10, 16, 15, 16, 17,  1, 12, 14, 14, 18,  1, 14,  6, 17, 10, 17,\n",
              "        14, 11, 18,  6, 17, 18,  5,  4, 15, 18,  9, 17, 15, 10,  7, 11,  9,  4,\n",
              "         1, 15,  1, 12, 10,  6, 15, 10,  9,  1, 16, 15, 16, 16, 10,  1, 18,  3,\n",
              "        15, 16,  3,  3, 14,  3, 16, 14,  1,  6,  9, 18,  3,  6,  4,  3,  1, 17,\n",
              "        18,  4, 17, 18, 16,  3,  1, 17, 13, 16,  4, 16, 18,  3,  9,  1,  6,  6,\n",
              "         6,  9, 10, 17, 10,  9,  3, 10,  4,  7, 15,  9, 16,  5,  3,  3, 16,  7,\n",
              "        11,  6,  3,  6, 13, 17, 11, 18, 16, 14, 18,  9, 13, 11,  9,  9,  1, 16,\n",
              "        15, 16, 18, 16,  4, 14, 16, 10, 14,  4, 16, 13, 14,  3,  4, 17, 13,  4,\n",
              "        16, 10,  3,  1,  4,  3,  4,  1, 20,  8, 15,  3,  1, 20, 10, 18,  5, 15,\n",
              "        16,  5,  2, 12, 10, 16, 17,  9,  8, 10, 13, 13,  3, 14, 10, 20,  3, 10,\n",
              "        15,  6, 14, 13, 11, 15, 16,  9, 10,  8, 16, 10,  7, 10,  8,  7, 17, 10,\n",
              "        10, 12, 12,  7,  8, 17, 18,  5, 17, 16, 13, 10,  2, 17,  8, 15, 12, 17,\n",
              "         9, 12, 12,  4, 13, 17, 12,  5, 10, 14,  1,  8,  9, 20, 20, 10,  2, 10,\n",
              "        16,  8, 17, 15, 12,  6,  1, 16, 16, 18,  3, 15, 18,  5,  3,  8,  2,  2,\n",
              "         4,  8,  5, 19, 10, 11, 10,  9, 20, 11, 15, 16, 16,  5,  9, 12,  4,  8,\n",
              "         4, 18,  5, 10, 12,  4,  8, 20, 10,  1, 10, 10,  1, 15, 15, 12,  1, 13,\n",
              "        10, 16, 14,  9, 10, 17,  5, 18,  6,  8, 10,  9, 15, 10,  2,  4,  3, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "hrC0f3LFBEGv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Same Architecture Encoder:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w4da4sYsBIiF",
        "outputId": "365645eb-8761-47b8-e0b7-87e361d4a684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class EncoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self, BATCH_SIZE = 64, ENCODING_LSTM_OUTPUT=200, CODE_LAYER_SIZE=300, VOCAB_SIZE=21):\n",
        "        super(EncoderNet, self).__init__()\n",
        "        #encoding\n",
        "        self.encoder = nn.LSTM(input_size=VOCAB_SIZE, hidden_size= ENCODING_LSTM_OUTPUT,num_layers=1,bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "      \n",
        "        # not clear to me if I want to have the latent space be repeated and given to every timestep of the LSTM?\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        res = torch.nn.utils.rnn.pack_padded_sequence(input, seq_len, batch_first=True).cuda()\n",
        "        res, (hidden_h, hidden_c) = self.encoder(res)\n",
        "        \n",
        "        return torch.cat( (hidden_h[0,:,:],hidden_h[1,:,:]), dim=1)\n",
        "\n",
        "encoder_net = EncoderNet()\n",
        "print(encoder_net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderNet(\n",
            "  (encoder): LSTM(21, 200, batch_first=True, bidirectional=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "voSLIQMjBU0-",
        "outputId": "1b091343-17b3-42c3-83ba-33677b303592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "class DecoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self,CODE_LAYER_SIZE=300, VOCAB_SIZE=21, FIRST_LSTM_NUM_LAYERS=1, hidden_size=200):\n",
        "        super(DecoderNet, self).__init__()\n",
        "        #decode\n",
        "        #self.dense1_predecode = nn.Linear(in_features=(CODE_LAYER_SIZE), out_features=50 )\n",
        "        self.decoder = nn.LSTM(input_size=(CODE_LAYER_SIZE*2),bidirectional=True, hidden_size=hidden_size,num_layers=1, batch_first=True)\n",
        "        self.dense1_dec = nn.Linear(in_features=400, out_features=100)\n",
        "        self.dense2_dec = nn.Linear(in_features=100, out_features=VOCAB_SIZE )\n",
        "        self.FIRST_LSTM_NUM_LAYERS=FIRST_LSTM_NUM_LAYERS\n",
        "        self.CODE_LAYER_SIZE=CODE_LAYER_SIZE\n",
        "        self.VOCAB_SIZE=VOCAB_SIZE\n",
        "        self.hidden_size=hidden_size\n",
        "\n",
        "    def forward(self, latent_space, hidden, seq_len):\n",
        "        #decoding. takes in a single latent code from a single part of the batch. \n",
        "        # where input is the teacher forcing or the predictions from the previous step.\n",
        "        \n",
        "        #prev_out = torch.nn.utils.rnn.pack_padded_sequence(latent_space, seq_len, batch_first=True).cuda()\n",
        "        prev_out, hidden = self.decoder( latent_space, hidden) \n",
        "        #prev_out, _ = torch.nn.utils.rnn.pad_packed_sequence(prev_out)\n",
        "        prev_out = F.elu(self.dense1_dec(prev_out))\n",
        "        prev_out = F.log_softmax(self.dense2_dec(prev_out), dim=2)\n",
        "        return prev_out\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        return (torch.zeros([self.FIRST_LSTM_NUM_LAYERS,batch_size,int(self.hidden_size)], device=device),\n",
        "                torch.zeros([self.FIRST_LSTM_NUM_LAYERS,batch_size,int(self.hidden_size)], device=device))\n",
        "\n",
        "    def initHidden_Out(self, batch_size):\n",
        "        return (torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device),\n",
        "                torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device))\n",
        "\n",
        "decoder_net = DecoderNet()\n",
        "print(decoder_net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderNet(\n",
            "  (decoder): LSTM(600, 200, batch_first=True, bidirectional=True)\n",
            "  (dense1_dec): Linear(in_features=400, out_features=100, bias=True)\n",
            "  (dense2_dec): Linear(in_features=100, out_features=21, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKRNKTFbqBSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Other Random functions"
      ]
    },
    {
      "metadata": {
        "id": "9UcLAsSzHQNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pretty_size(size):\n",
        "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
        "\tassert(isinstance(size, torch.Size))\n",
        "\treturn \" × \".join(map(str, size))\n",
        "\n",
        "def dump_tensors(gpu_only=True):\n",
        "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
        "\timport gc\n",
        "\ttotal_size = 0\n",
        "\tfor obj in gc.get_objects():\n",
        "\t\ttry:\n",
        "\t\t\tif torch.is_tensor(obj):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
        "\t\t\t\t\ttotal_size += obj.numel()\n",
        "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
        "\t\t\t\t\ttotal_size += obj.data.numel()\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tpass        \n",
        "\tprint(\"Total size:\", total_size)\n",
        "    \n",
        "dump_tensors(gpu_only=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rPQEOqkp3vN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3uudPpYtXA_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Useful code to take: "
      ]
    },
    {
      "metadata": {
        "id": "lr-4-Nh5bMuy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# interesting! \n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0IAO3dPbW_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "omL-ksZOfAI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n38EHkYUjvat",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Amino Acids to integers\n",
        "\n",
        "I have saved the vocabulary index also! \n",
        "\n",
        "## Beware that I have added one to everything!!!!!! When working with the data. As the padding value is 0. (previously... I am not using padding any more though!!)"
      ]
    },
    {
      "metadata": {
        "id": "W6JsjAcG_deK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "primary_train = pickle.load(open('training_primary.pickle', 'rb'))\n",
        "primary_test = pickle.load(open('testing_primary.pickle', 'rb'))\n",
        "primary_cv = pickle.load(open('validation_primary.pickle', 'rb'))\n",
        "#pickle.dump(primary, open('training_primary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3OCIWA5ECxgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "for seq in primary_train:\n",
        "  #temp = set((list(seq)))\n",
        "  vocab.update( set((list(seq))) )\n",
        "print(len(list(vocab)))\n",
        "# => ['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']\n",
        "vocab = sorted(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDYSm8byARQ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#vocab = sorted(set([  c for c in seqs for seqs in primary_train])) #sorted(set([char for seq in primary for char in primary_train]))\n",
        "def chars_to_ints(sequences, vocab):\n",
        "  return np.asarray( [np.asarray(s) for s in [[vocab.index(tok) for tok in seq] for seq in sequences]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5CTaSDPDfRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "primary_train = chars_to_ints(primary_train, vocab)\n",
        "primary_cv  = chars_to_ints(primary_cv, vocab)\n",
        "primary_test  = chars_to_ints(primary_test, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23JgLWytCSrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "primary_test.shape "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WI2PfWwCEUmF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pickle.dump(vocab, open('sorted_vocab_encoding.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HLs3a0OEDKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''pickle.dump(primary_train, open('training_primary_ints.pickle', 'wb'))\n",
        "pickle.dump(primary_test, open('testing_primary_ints.pickle', 'wb'))\n",
        "pickle.dump(primary_cv, open('validation_primary_ints.pickle', 'wb'))'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLh5EYSZPpur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Teacher Forcing Friendly Encoder and Decoder. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "414fc08a-b02d-418c-dc95-553d2b7825fe",
        "id": "AaP9K6X2Pt0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class EncoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self, ENCODING_LSTM_OUTPUT=100, CODE_LAYER_SIZE=50, VOCAB_SIZE=20, ENCODER_LSTM_NUM_LAYERS=2 ):\n",
        "        super(EncoderNet, self).__init__()\n",
        "        #encoding\n",
        "        self.LSTM_NUM_LAYERS = ENCODER_LSTM_NUM_LAYERS\n",
        "        self.ENCODING_LSTM_OUTPUT = ENCODING_LSTM_OUTPUT\n",
        "        self.encoder = nn.LSTM(input_size=VOCAB_SIZE, hidden_size= self.ENCODING_LSTM_OUTPUT,num_layers=self.LSTM_NUM_LAYERS,bidirectional=True, batch_first=True)\n",
        "        #self.batchnorm = nn.BatchNorm1d(ENCODING_LSTM_OUTPUT * 2* self.LSTM_NUM_LAYERS) # as either 2 layers OR I concat them\n",
        "        self.dense1_enc = nn.Linear(in_features=(ENCODING_LSTM_OUTPUT* 2 ), out_features=CODE_LAYER_SIZE ) # * self.LSTM_NUM_LAYERS because it is bidirectional\n",
        "        self.dense2_enc = nn.Linear(in_features=CODE_LAYER_SIZE, out_features=CODE_LAYER_SIZE )\n",
        "        if self.LSTM_NUM_LAYERS >2:\n",
        "            print('WARNING, LSTM ENCODER HAS MORE THAN 2 LAYER, will need to change how hidden layers are concatenated!')\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        #print('encoder input', input)\n",
        "        #print('sequence length', seq_len)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        #res = torch.nn.utils.rnn.pack_padded_sequence(input, seq_len, batch_first=True).cuda()\n",
        "        res, (hidden_h, hidden_c) = self.encoder(input)\n",
        "        #res = torch.nn.utils.rnn.pad_packed_sequence(res, batch_first=True).cuda() # only use if I want the output w/ all timesteps for the LSTM\n",
        "        # combine the bidirectional outputs, concatenate states from separate layers. \n",
        "        #print('hidden encoder that i concat', hidden_h.shape)\n",
        "        \n",
        "        res = torch.mean(res, dim=1)\n",
        "        \n",
        "        #res = res.view(self.LSTM_NUM_LAYERS, 2, input.shape[0], self.ENCODING_LSTM_OUTPUT)\n",
        "        \n",
        "        '''if self.LSTM_NUM_LAYERS ==2:\n",
        "            #res = hidden_h.view(self.LSTM_NUM_LAYERS, 2, input.shape[0], self.ENCODING_LSTM_OUTPUT)\n",
        "            \n",
        "            # may be much more efficient ways to do this:\n",
        "            res = torch.cat( (torch.cat( (res[0,0,:,:], res[0,1,:,:]) ,1), torch.cat( (res[1,0,:,:], res[1,1,:,:]) ,1) ), 1)\n",
        "            #res= torch.cat( (torch.add(torch.mean(res[0,:,:], dim=),hidden_h[1,:,:]),torch.add(hidden_h[2,:,:],hidden_h[3,:,:])), 1)\n",
        "        else: \n",
        "            # average over all of the hidden states!\n",
        "            print(res.shape)\n",
        "            # res = torch.mean( res, dim=1)\n",
        "            torch.cat((hidden_h[0,:,:],hidden_h[1,:,:]), 1) '''\n",
        "        #res = self.batchnorm(res)\n",
        "        res = self.dense2_enc(F.elu(self.dense1_enc(res))) # used to have F.tanh here!\n",
        "        return res\n",
        "\n",
        "encoder_net = EncoderNet()\n",
        "print(encoder_net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderNet(\n",
            "  (encoder): LSTM(20, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (dense1_enc): Linear(in_features=200, out_features=50, bias=True)\n",
            "  (dense2_enc): Linear(in_features=50, out_features=50, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "ffc3b7b9-ff6d-423d-e512-843b937684be",
        "id": "5CTNuIWXPt0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "class DecoderNet(nn.Module):\n",
        "\n",
        "    def __init__(self, DECODING_LSTM_OUTPUT=100, CODE_LAYER_SIZE=50, VOCAB_SIZE=20, DECODER_LSTM_NUM_LAYERS=1 ):\n",
        "        super(DecoderNet, self).__init__()\n",
        "        #decode it should be the inverse of the encoder!\n",
        "        #self.dense1_predecode = nn.Linear(in_features=(CODE_LAYER_SIZE), out_features=50 )\n",
        "        #self.dense1_pre_dec = nn.Linear(in_features=(CODE_LAYER_SIZE), out_features=200 )\n",
        "        self.LSTM_OUTPUT_SIZE = DECODING_LSTM_OUTPUT\n",
        "        #self.decoder_out = nn.LSTM(input_size=int(CODE_LAYER_SIZE/2),bidirectional=False, hidden_size=VOCAB_SIZE,num_layers=1, batch_first=True)\n",
        "        self.LSTM_NUM_LAYERS=DECODER_LSTM_NUM_LAYERS\n",
        "        self.CODE_LAYER_SIZE=CODE_LAYER_SIZE\n",
        "        self.VOCAB_SIZE=VOCAB_SIZE\n",
        "        self.decoder = nn.LSTM(input_size=( (CODE_LAYER_SIZE)+VOCAB_SIZE),bidirectional=False, #I NEED TO BE PASSING IN THE WHOLE ONE HOT VECTOR AGAIN!\n",
        "                               hidden_size=self.LSTM_OUTPUT_SIZE,num_layers=self.LSTM_NUM_LAYERS, batch_first=True)\n",
        "        # should be half the size but the LSTM is bidirectional. \n",
        "        #self.batchnorm = nn.BatchNorm1d(self.LSTM_OUTPUT_SIZE)\n",
        "        self.dense1_post_dec = nn.Linear(in_features=(self.LSTM_OUTPUT_SIZE), out_features=50 )\n",
        "        self.dense2_post_dec = nn.Linear(in_features=50, out_features=VOCAB_SIZE )\n",
        "        #self.dense3_post_dec = nn.Linear(in_features=50, out_features=VOCAB_SIZE )\n",
        "\n",
        "    def forward(self, latent_space, prev_out, hidden):\n",
        "        #decoding. takes in a single latent code from a single part of the batch. \n",
        "        # where input is the teacher forcing or the predictions from the previous step.\n",
        "\n",
        "        prev_out, hidden = self.decoder( torch.cat( (latent_space, prev_out ), 2 ), hidden)\n",
        "        \n",
        "        #print('batch norm ', prev_out.shape)\n",
        "        #prev_out = self.batchnorm(prev_out.permute([0,2,1])).permute([0,2,1]).contiguous()\n",
        "        #print('batch norm ', prev_out.shape)\n",
        "        prev_out = F.elu(self.dense1_post_dec(prev_out))\n",
        "        prev_out = F.log_softmax(self.dense2_post_dec(prev_out), dim=2)\n",
        "        return prev_out, hidden\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        hidden_h = torch.empty([self.LSTM_NUM_LAYERS,batch_size,int(self.LSTM_OUTPUT_SIZE)], device=device)\n",
        "        torch.nn.init.orthogonal_(hidden_h, gain=nn.init.calculate_gain('tanh'))\n",
        "        return (hidden_h,\n",
        "                torch.zeros([self.LSTM_NUM_LAYERS,batch_size,int(self.LSTM_OUTPUT_SIZE)], device=device))\n",
        "\n",
        "'''   def initHidden_Out(self, batch_size):\n",
        "        return (torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device),\n",
        "                torch.zeros([1,batch_size,self.VOCAB_SIZE], device=device))'''\n",
        "\n",
        "decoder_net = DecoderNet()\n",
        "print(decoder_net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderNet(\n",
            "  (decoder): LSTM(51, 100, batch_first=True)\n",
            "  (batchnorm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dense1_post_dec): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (dense2_post_dec): Linear(in_features=50, out_features=20, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hpkyGaaig9-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Original Bucket by Seq Length and padder:"
      ]
    },
    {
      "metadata": {
        "id": "ak_M9SPjqMUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "class BySequenceLengthSampler(Sampler):\n",
        "\n",
        "    def __init__(self, data_source,  \n",
        "                bucket_boundaries, batch_size=64,):\n",
        "        ind_n_len = []\n",
        "        for i, p in enumerate(data_source):\n",
        "            ind_n_len.append( (i, p.shape[0]) )\n",
        "        self.ind_n_len = ind_n_len\n",
        "        self.bucket_boundaries = bucket_boundaries\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        \n",
        "    def __iter__(self):\n",
        "        print('bucketing the data again!!! ===========')\n",
        "        data_buckets = dict()\n",
        "        # where p is the id number and seq_len is the length of this id number. \n",
        "        record = []\n",
        "        for p, seq_len in self.ind_n_len:\n",
        "            pid = self.element_to_bucket_id(p,seq_len)\n",
        "            if pid in data_buckets.keys():\n",
        "                data_buckets[pid].append(p)\n",
        "            else:\n",
        "                data_buckets[pid] = [p]\n",
        "\n",
        "        for k in data_buckets.keys():\n",
        "            data_buckets[k] = np.asarray(data_buckets[k])\n",
        "            record.append((k , data_buckets[k].shape[0]))\n",
        "        print(record)\n",
        "        iter_list = []\n",
        "        for k in data_buckets.keys():\n",
        "            np.random.shuffle(data_buckets[k])\n",
        "            iter_list += (np.array_split(data_buckets[k]\n",
        "                           , int(data_buckets[k].shape[0]/self.batch_size)))\n",
        "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
        "        # size\n",
        "        for i in iter_list: \n",
        "            yield i.tolist() # as it was stored in an array\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "    \n",
        "    def element_to_bucket_id(self, x, seq_length):\n",
        "        boundaries = list(self.bucket_boundaries)\n",
        "        buckets_min = [np.iinfo(np.int32).min] + boundaries\n",
        "        buckets_max = boundaries + [np.iinfo(np.int32).max]\n",
        "        conditions_c = np.logical_and(\n",
        "          np.less_equal(buckets_min, seq_length),\n",
        "          np.less(seq_length, buckets_max))\n",
        "        bucket_id = np.min(np.where(conditions_c))\n",
        "        return bucket_id\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfZz3TO8PJ_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sort_batch(batch, lengths):\n",
        "    \"\"\"\n",
        "    Sort a minibatch by the length of the sequences with the longest sequences first\n",
        "    return the sorted batch targes and sequence lengths.\n",
        "    This way the output can be used by pack_padded_sequences(...)\n",
        "    \"\"\"\n",
        "    seq_lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "    seq_tensor = batch[perm_idx]\n",
        "    return seq_tensor, seq_lengths\n",
        "\n",
        "def pad_and_sort_batch(DataLoaderBatch):\n",
        "    \"\"\"\n",
        "    DataLoaderBatch should be a list of (sequence, target, length) tuples...\n",
        "    Returns a padded tensor of sequences sorted from longest to shortest, \n",
        "    \"\"\"\n",
        "    vocab_size = 21\n",
        "    batch_size = len(DataLoaderBatch)\n",
        "    batch_split = list(zip(*DataLoaderBatch))\n",
        "\n",
        "    seqs, lengths = batch_split[0], batch_split[1]\n",
        "    max_length = max(lengths)\n",
        "    \n",
        "    padded_seqs = np.zeros((batch_size, max_length))\n",
        "    for i, l in enumerate(lengths):\n",
        "        padded_seqs[i, 0:l] = seqs[i][0:l]\n",
        "    \n",
        "    #print(padded_seqs)\n",
        "    #print('seq') write it as sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)\n",
        "    y = torch.tensor(padded_seqs).type(torch.LongTensor) #had device\n",
        "    y_onehot = torch.FloatTensor(batch_size, max_length, vocab_size)\n",
        "    y_onehot.zero_()\n",
        "    y_onehot.scatter_(2, torch.unsqueeze(y, 2), 1.0)\n",
        "    #print('max')\n",
        "    #print(y_onehot.max(dim=2)[1])\n",
        "    return sort_batch(y_onehot, torch.tensor(lengths)) # had device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3PkICOAjlxsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bucket_boundaries = [50,75,100]#,125,150,175,200,225,250,275,300,325,350,375,400,425,450,475,500] #,700]#,800,900,1000,1100,1200]\n",
        "    sampler = BySequenceLengthSampler(primary_train,bucket_boundaries, BATCH_SIZE)\n",
        "    dataloader = DataLoader(df_train, batch_size=1, \n",
        "                            batch_sampler=sampler, shuffle=False,\n",
        "                            num_workers=8, collate_fn=pad_and_sort_batch, \n",
        "                            drop_last=False, pin_memory=mem_pin)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}